# Machine-Learning-Lab-6-Programs
A1. Develop the above perceptronin your own code (don’t use the perceptron model available from package). Use the initial weights as provided below. W0 = 10, W1 = 0.2, w2 = -0.75, learning rate (α) = 0.05Use Stepactivation function to learn the weights of the network to implement above provided AND gate logic. The activation function is demonstratedbelow.
Identify the number of epochs needed for the weights to converge in the learning process. Make a plot of the epochs against the error values calculated (after each epoch, calculate the sum-square-error against all training samples). (Note: Learning is said to be converged if the error is less than or equal to 0.002. Stop the learning after 1000 iterations if the convergence error condition is not met.)
A2. Repeat the above A1 experiment with following activation functions. Compare the iterations taken to converge against each of the activation functions. Keep the learning rate same as A1.•Bi-Polar Step function•Sigmoid function•ReLU function
A3. Repeat exercise A1 with varying the learning rate, keeping the initial weights same. Take learning rate = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}. Make a plot of the number of iterations taken for learning to converge against the learning rates.
A4. Repeat the above exercises, A1 to A3,for XOR gate logic.
A5. Use customer data provided below. Build a perceptron & learn to classify the transactions as high or low value as provided in the below table. Use sigmoid as the activation function. Initialize the weights & learning rate with your choice.CustomerCandies (#)Mangoes (Kg)Milk Packets (#)Payment (Rs)High Value Tx?C_12062386YesC_21636289YesC_32762393YesC_41912110NoC_52442280YesC_62215167NoC_71542271YesC_81842274YesC_92114148NoC_101624198No
A6. Compare the results obtained from above perceptron learning to the ones obtained with matrix pseudo-inverse.
A7. Develop the below Neural Network. Use learning rate (α) = 0.05 with a Sigmoid activation function. Learn the weights of the network using back-propagation algorithm to implement above provided AND gate logic. (Note: Learning is said to be converged if the error is less than or equal to 0.002. Stop the learning after 1000 iterations if the convergence error condition is not met.Logic for back-propagation is provided below.)
A8. Repeat the above A1 experiment for XOR Gate logic. Keep the learning rate & activation function same as A1.
(Content obtained from Tom Mitchell book.)
A9. Repeat exercise A1 & A2 with 2 output nodes (as shown below). A zero output of logic gate maps to [O1O2] = [1 0] from output layer while a one output from logic gate maps to [0 1]. A10. Learn using a MLP network from Sci-Kit manual available at https://scikit-learn.org/stable/modules/neural_networks_supervised.html. Repeat the AND Gate and XOR Gate exercises using MLPClassifier() function.A11. Use the MLPClassifier() function on your project dataset
